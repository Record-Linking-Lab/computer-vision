# Train Configuration File

# The path to a tab-delimited CSV file containing training data information formatted as | IMG_PATH | Transcription |
train_csv_path: V:\FHSS-JoePriceResearch\data\wilford_woodruff\train_list.csv
# The path to a tab-delimited CSV file containing validation data information formatted as | IMG_PATH | Transcription |
val_csv_path: V:\FHSS-JoePriceResearch\data\wilford_woodruff\train_list.csv
# The ratio used to determine the size of the train/validation split. If split_train_size is set to 0.8, then the
# training set will contain 80% of the data, and validation 20%. The dataset is not shuffled before being split.
split_train_size: 0.8

# Whether or not to apply the noise augmentation to the training dataset
apply_noise_augmentation: False
# Whether or not to apply the bleedthrough augmentation to the training dataset
apply_bleedthrough_augmentation: False
# Whether or not to apply the grid warp augmentation to the training dataset
apply_grid_warp_augmentation: True
# The interval in pixels between control points in the grid warp augmentation
grid_warp_interval: 16
# The standard deviation required in the grid warp augmentation
grid_warp_stddev: 2

# The path to store the trained model weights
model_out: V:\FHSS-JoePriceResearch\data\wilford_woodruff\HandwritingRecognition-master
# The path to pre-trained model weights
model_in:

# The number of epochs to train
epochs: 50
# The number of images in a mini-batch
batch_size: 10
# The learning rate the optimizer uses during training
learning_rate: 0.001  # DO NOT use scientific notation! Pyyaml interprets scientific notation as string.
# The max number of characters in a line-level transcription
max_seq_size: 128
# The size which all images will be resized - (height, width)
img_size: (64, 1024)

# String including the character set for the model (charset: abcd1234) If no characters are specified, the default is used.
charset: